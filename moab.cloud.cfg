################################################################################
#
#  Moab Configuration File for moab-7.2.10
#
#  Documentation can be found at
#  http://docs.adaptivecomputing.com/mwm/7-1-3/help.htm
#
#  For a complete list of all parameters (including those below) please see:
#  http://docs.adaptivecomputing.com/mwm/7-1-3/help.htm#a.fparameters.html
#
#  For more information on the initial configuration, please refer to:
#  http://docs.adaptivecomputing.com/mwm/7-1-3/help.htm#topics/installation/initialconfig.html
#
#  Use 'mdiag -C' to validate config file parameters
#
################################################################################

SCHEDCFG[Moab]        SERVER=pl-torque.dccn.nl:42559 MODE=NORMAL
ADMINCFG[1]           NAME=admin USERS=root GROUPS=tg SERVICES=ALL
ADMINCFG[5]           NAME=users USERS=ALL SERVICES=NONE
TOOLSDIR              /usr/local/moab/tools

################################################################################
#
#  Resource Manager configuration
#
#  For more information on configuring a Resource Manager, see:
#  http://www.adaptivecomputing.com/resources/docs/mwm/6-0/13.2rmconfiguration.php
#
#  For more information on configuring a Native Resource Manager with GMETRICS, see:
#  http://docs.adaptivecomputing.com/mwm/Content/topics/nodeAdministration/enablinggmetrics.html 
#
################################################################################

RMCFG[torque]  TYPE=PBS HOST=pl-torque.dccn.nl PORT=15001 TIMEOUT=300 SUBMITCMD=/usr/bin/qsub
RMCFG[torque]  JOBRSVRECREATE=FALSE
#RMCFG[gpu]         TYPE=NATIVE FLAGS=static CLUSTERQUERYURL=file://$TOOLSDIR/dccn/rmgr_gpu_metrics.cfg
#RMCFG[memutil]     TYPE=NATIVE CLUSTERQUERYURL=exec://$TOOLSDIR/dccn/rmgr_memory_util.sh
RMCFG[matlablic]    TYPE=NATIVE RESOURCETYPE=LICENSE CLUSTERQUERYURL=exec://$TOOLSDIR/dccn/rmgr_matlab_lic.sh
RMCFG[licmon]   TYPE=NATIVE CLUSTERQUERYURL=exec://$TOOLSDIR/dccn/rmgr_matlab_lic_node.sh

################################################################################
#
# Performance tuning
# 
#  - http://docs.adaptivecomputing.com/mwm/7-2-10/help.htm#a.ilargeclusters.html
#
################################################################################

RMPOLLINTERVAL        00:00:45
JOBAGGREGATIONTIME    00:00:15

# fork process to listen on client commands, e.g. checkjob, showq, etc.
UIMANAGEMENTPOLICY    FORK
CLIENTUIPORT          41560

# log file management 
LOGFILE               moab.log
LOGFILEMAXSIZE        10000000
LOGLEVEL              1
LOGFILEROLLDEPTH      8

# max. number of jobs moab will consider for scheduling
MAXJOB                20000

################################################################################
#
#  Database configuration
#
#  For more information on Moab's integrated database see:
#  http://www.adaptivecomputing.com/resources/docs/mwm/6-0/a.fparameters.php#usedatabase
#
################################################################################

USEDATABASE        INTERNAL

# Job Priority: http://www.adaptivecomputing.com/resources/docs/maui/5.1jobprioritization.php
# a value of 1 adds one priority point per minute

QUEUETIMEWEIGHT       1

# FairShare: http://www.adaptivecomputing.com/resources/docs/maui/6.3fairshare.php
# For a blank user, a positive priority of 15% times 9600 points will be given,
# which corresponds to one day of QUEUETIMEWEIGHT. For a historical 100%
# user, a negative priority of 85% times 9600 is appriximately 5.6 days
# in QUEUETIMEWEIGHT will be given.

FSPOLICY               [NONE]     # Disabled after decided HPC-meeting 2016-04-07
#FSPOLICY              DEDICATEDPS     # Disabled after decided HPC-meeting 2016-04-07
#FSDEPTH               7               # Disabled after decided HPC-meeting 2016-04-07 
#FSINTERVAL            24:00:00        # Disabled after decided HPC-meeting 2016-04-07
#FSDECAY               0.75            # Disabled after decided HPC-meeting 2016-04-07
#FSWEIGHT              9600            # Disabled after decided HPC-meeting 2016-04-07
#FSUSERWEIGHT          1               # Disabled after decided HPC-meeting 2016-04-07
#FSGROUPWEIGHT         1               # Disabled after decided HPC-meeting 2016-04-07

# Increase the target from 15% to 20% to avoid unnecessary punishment to a user who
# contribute the total cluster usage for 1 day.
#  - Decision from torque/maui meeting on 12 June 2014
#USERCFG[DEFAULT]      FSTARGET=20.0   # Disabled after decided HPC-meeting 2016-04-07

# Priority of interactive queue
#  - any interactive job priority should be above other priority factors coming from the FS
#CLASSCFG[DEFAULT]	PLIST=production
CLASSCFG[DEFAULT]	PARTITION=production PLIST=production
CLASSCFG[interactive]	PRIORITY=100
CLASSWEIGHT 2000

# Throttling Policies: http://www.adaptivecomputing.com/resources/docs/maui/6.2throttlingpolicies.php
# MAXPS  Limits the number of outstanding processor-seconds a credential may have allocated at any given time
# MAXMEM Limits the total amount of dedicated memory (in MB) which can be allocated by a credential's active jobs at any given time.

#USERCFG[DEFAULT]	MAXJOB=220 MAXIJOB=200 MAXMEM=563200 MAXPS=5184000

## allow single user to run 220 concurrent jobs, each requires 4 GB and 72 hours walltime
USERCFG[DEFAULT]	MAXJOB=300 MAXIJOB=200 MAXMEM=1126400 MAXPS=57024000
USERWEIGHT 1000 

## on-demand hard-limit requests for specific users
#USERCFG[nilmul]	MAXJOB=300 MAXIJOB=200 MAXMEM=2252800 MAXPS=57024000
#USERCFG[jansch]	MAXJOB=300 MAXIJOB=200 MAXMEM=4505600 MAXPS=57024000
#USERCFG[natbie]	MAXJOB=300 MAXIJOB=200 MAXMEM=4505600 MAXPS=57024000
#USERCFG[iriiki]	MAXJOB=300 MAXIJOB=200 MAXMEM=4505600 MAXPS=57024000
#USERCFG[koehaa]	MAXJOB=300 MAXIJOB=200 MAXMEM=4505600 MAXPS=57024000
#USERCFG[zahfaz]	MAXJOB=300 MAXIJOB=200 MAXMEM=4505600 MAXPS=57024000

# Backfill: http://www.adaptivecomputing.com/resources/docs/maui/8.2backfill.php

BACKFILLPOLICY		FIRSTFIT
RESERVATIONPOLICY	CURRENTHIGHEST

NODEACCESSPOLICY        SHARED

# Node availability: http://docs.adaptivecomputing.com/mwm/7-2-9/help.htm#topics/prio_res/nodeavailability.html
#  - node is considered "busy" if
#    * number of jobs     >= total number of CPUs on the node  
#    * memory utilisation >= total memory of the node
NODEAVAILABILITYPOLICY  DEDICATED:PROC UTILIZED:MEM

# Node Allocation: http://docs.adaptivecomputing.com/mwm/7-2-9/help.htm#topics/prio_res/nodeallocation.html 
#  - favor the fastest nodes
#  - for MATLAB queue, favor the nodes with "matlab" feature and many cores
NODEALLOCATIONPOLICY    PRIORITY
NODECFG[DEFAULT] PRIORITYF='SPEED'
CLASSCFG[matlab] PRIORITYF='FEATURE[matlab] + 0.01*CPROCS'

# Deferred job handling
DEFERTIME 00:10:00
DEFERSTARTCOUNT 3

# QOS with job preemption: http://www.adaptivecomputing.com/resources/docs/maui/7.3qos.php
#
# QOSCFG[hi]  PRIORITY=100 XFTARGET=100 FLAGS=PREEMPTOR:IGNMAXJOB
# QOSCFG[low] PRIORITY=-1000 FLAGS=PREEMPTEE
# CLASSCFG[batch]       FLAGS=PREEMPTEE
# CLASSCFG[interactive] FLAGS=PREEMPTOR

# machines with a relative speed of 0.5 can allow for twice the walltime
# at this moment it is overruled by the torque queue configuration
# USEMACHINESPEED ON

NODECFG[DEFAULT]	SPEED=1.00 # this is for Intel(R) Xeon(R) CPU W3530 @ 2.80GHz
NODECFG[dccn-c004.dccn.nl]	SPEED=0.62 GRES=bandwidth:1000 PARTITION=production
NODECFG[dccn-c005.dccn.nl]	SPEED=0.90 GRES=bandwidth:1000 PARTITION=production
NODECFG[dccn-c006.dccn.nl]	SPEED=0.40 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c007.dccn.nl]	SPEED=0.40 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c010.dccn.nl]	SPEED=0.45 GRES=bandwidth:1000 PARTITION=production
NODECFG[dccn-c011.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c012.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c013.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c014.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c015.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c016.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c017.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c018.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c019.dccn.nl]	SPEED=0.41 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c020.dccn.nl]	SPEED=0.41 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c021.dccn.nl]	SPEED=0.41 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c022.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c023.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c024.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c025.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c026.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c027.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c028.dccn.nl]	SPEED=0.95 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c029.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production 
NODECFG[dccn-c030.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c031.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c032.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c033.dccn.nl]	SPEED=0.42 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c034.dccn.nl]	SPEED=0.42 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c035.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c036.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c037.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c038.dccn.nl]	SPEED=0.99 GRES=bandwidth:10000 PARTITION=production
NODECFG[dccn-c350.dccn.nl]	PARTITION=test GRES=bandwidth:1000
NODECFG[dccn-c351.dccn.nl]	PARTITION=test GRES=bandwidth:1000
NODECFG[dccn-c352.dccn.nl]	PARTITION=test GRES=bandwidth:1000
NODECFG[dccn-c353.dccn.nl]	PARTITION=test GRES=bandwidth:1000
NODECFG[dccn-c354.dccn.nl]	PARTITION=test GRES=bandwidth:1000
NODECFG[dccn-c360.dccn.nl]	PARTITION=production GRES=bandwidth:1000
NODECFG[dccn-c361.dccn.nl]	PARTITION=production GRES=bandwidth:1000
NODECFG[dccn-c362.dccn.nl]	PARTITION=production GRES=bandwidth:1000
NODECFG[dccn-c363.dccn.nl]	PARTITION=production GRES=bandwidth:1000
NODECFG[dccn-c364.dccn.nl]	PARTITION=production GRES=bandwidth:1000
NODECFG[dccn-c365.dccn.nl]	PARTITION=production GRES=bandwidth:1000

NODECFG[mentat999.dccn.nl]	SPEED=0.75


# Add a cluster partition and allow only the queue test to submit jobs
# to nodes attached to this partition
CLASSCFG[tgtest]			PARTITION=test PLIST=test


################################################################################
#
# Elastic Computing bursting out to SURFSara HPC cloud 
#
#  For more information on Moab's Elastic Computing see the Elastic Computing 
#  section in the Moab Workload Manager guide in the documentation link above.
#
################################################################################
# Add qos elastic to all users
USERCFG[DEFAULT]        QLIST=elastic

# Enable scheduling on elastic nodes
SCHEDCFG[Moab]		FLAGS=enabledynamicnodes

# This trigger is used by Moab to sync its internal elastic usage records with RLM
SCHEDCFG[Moab]		TRIGGER=atype=exec,action="$TOOLSDIR/rlm/sync_moab_rlm.py -f /opt/rlm/adaptiveco.log",etype=standing,period=minute,offset=10

QOSCFG[DEFAULT]		ENABLEPROFILING=TRUE
NODECFG[DEFAULT]	ENABLEPROFILING=TRUE

# Send script the request geometry of the highest priority job in the backlog
QOSCFG[elastic]		REQUESTGEOMETRY=PRIORITYJOBSIZE

# Create an elastic node if the back log completion time exceeds 500 processor seconds
QOSCFG[elastic]		TRIGGER=EType=threshold,AType=exec,TType=elastic,Action="$TOOLSDIR/dccn/elastic-hpccloud-mom-trigger.py deploy --jobid $JOBID --user $USER --request-geometry $REQUESTGEOMETRY",Threshold=BACKLOGCOMPLETIONTIME>500,RearmTime=01:00

# Remove nodes if they are idle for 120 seconds
NODEIDLEPURGETIME		120

# Remove elastic nodes when the TTL expires
NODECFG[DEFAULT]		TRIGGER=EType=end,AType=exec,TType=elastic,Action="$TOOLDIR/dccn/elastic-hpccloud-mom-trigger.py shutdown --node-name $OID"

# amount of time Maui will allow a job to exceed its wallclock limit before 
# it is terminated
# -1 stands for unlimited (edwger 18 apr 2012)
JOBMAXOVERRUN	-1
ENFORCERESOURCELIMITS ON
RESOURCELIMITPOLICY MEM:ALWAYS:CANCEL

# NEW standing reservation for interactive sessions during office hours
SRCFG[daytimeinteractive]   CLASSLIST=interactive
SRCFG[daytimeinteractive]   DAYS=MON,TUE,WED,THU,FRI
SRCFG[daytimeinteractive]   STARTTIME=8:30:00
SRCFG[daytimeinteractive]   ENDTIME=20:00:00
SRCFG[daytimeinteractive]   PERIOD=DAY
#SRCFG[daytimeinteractive]   HOSTLIST=dccn-c00[3-5],dccn-c00[8-9],dccn-c0[10-18],dccn-c02[2-4],dccn-c36[0-4]
SRCFG[daytimeinteractive]   HOSTLIST=dccn-c00[4-5],dccn-c0[10-18],dccn-c02[2-8],dccn-c36[0-4]
SRCFG[daytimeinteractive]   RESOURCES=PROCS:1;MEM:1024
SRCFG[daytimeinteractive]   TASKCOUNT=72

# standing reservation for interactive sessions during office hours
#SRCFG[daytimeinteractive]   CLASSLIST=interactive
#SRCFG[daytimeinteractive]   DAYS=MON,TUE,WED,THU,FRI
#SRCFG[daytimeinteractive]   STARTTIME=8:30:00
#SRCFG[daytimeinteractive]   ENDTIME=20:00:00
#SRCFG[daytimeinteractive]   PERIOD=DAY
#SRCFG[daytimeinteractive]   HOSTLIST=dccn-c00[8-9],dccn-c01[0-3]

# standing reservation for veryshort/short batch jobs during office hours
SRCFG[daytimebatch]   CLASSLIST=veryshort,short,vgl
SRCFG[daytimebatch]   DAYS=MON,TUE,WED,THU,FRI
SRCFG[daytimebatch]   STARTTIME=8:30:00
SRCFG[daytimebatch]   ENDTIME=20:00:00
SRCFG[daytimebatch]   PERIOD=DAY
SRCFG[daytimebatch]   HOSTLIST=dccn-c36[4-5]

# standing reservation for lcmgui interactive jobs
SRCFG[lcminteractive]   CLASSLIST=lcmgui
SRCFG[lcminteractive]   HOSTLIST=dccn-c032
SRCFG[lcminteractive]   RESOURCES=PROCS:1;MEM:3072
SRCFG[lcminteractive]   TASKCOUNT=4
SRCFG[lcminteractive]   PERIOD=INFINITY

# standing reservation for nodes in the centos7 test environment
SRCFG[test]   CLASSLIST=tgtest
SRCFG[test]   PARTITION=test
SRCFG[test]   HOSTLIST=dccn-c35[0-4]
SRCFG[test]   PERIOD=INFINITY
